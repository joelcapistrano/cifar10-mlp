{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 - MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joelcapistrano/cifar10-mlp/blob/master/CIFAR10_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpagxIzidkaz",
        "colab_type": "text"
      },
      "source": [
        "# **Multi Layer Perceptron for CIFAR10**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0NwreZX97F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxc6VEqZ-F5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-4Dc_bhsbv",
        "colab_type": "text"
      },
      "source": [
        "Load CIFAR10 dataset and prepare training/test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66_HM-iC-kHu",
        "colab_type": "code",
        "outputId": "f5a460dc-ce1c-4acb-9452-5494e55274b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Get number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# Convert labels to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Get image dimension (image is assumed to be square)\n",
        "image_size = x_train.shape[1]\n",
        "\n",
        "# For MLP, the input dim is a vector, so we reshape\n",
        "input_size = image_size * image_size * 3\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "\n",
        "# Normalize image values\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYh89Z_5hv98",
        "colab_type": "text"
      },
      "source": [
        "Set network parameters that will be used by MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz27RNh-iDD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "hidden_units = [256,128,64]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpadK-yHiGNY",
        "colab_type": "text"
      },
      "source": [
        "The MLP Architecture has **three Dense layers** with **ReLU** activation function added after each Dense layer. A function is set up to facilitate **hyperparameter tuning** (number of nodes per dense layer). **Categorical Crossentropy** is chosen as the loss function and **Accuracy** is chosen as the metric since the CNN will perform single label classification (i.e. only one label can be correct)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpzNeAx4qmk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MLP Architecture\n",
        "# Dense-Dense-Dense-Dense-Activation\n",
        "# Batch Size set to 128\n",
        "# 20 Epochs for each training/tuning run\n",
        "def run_model(hidden_units):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(hidden_units[0], input_dim=input_size))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(hidden_units[1]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(hidden_units[2]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(num_labels))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='sgd',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
        "  train_score = model.evaluate(x_train, y_train, batch_size=128)\n",
        "  print(\"\\nTrain accuracy: %.1f%%\" % (100.0 * train_score[1]))\n",
        "  test_score = model.evaluate(x_test, y_test, batch_size=128)\n",
        "  print(\"\\nTest accuracy: %.1f%%\" % (100.0 * test_score[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pwE26Fo9if",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a05addd-9750-422c-ad7e-a262f3b24364"
      },
      "source": [
        "# Set hidden units to 256-128-64\n",
        "run_model([256,128,64])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 828,490\n",
            "Trainable params: 828,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 2.0150 - acc: 0.2765\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.8453 - acc: 0.3445\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 1.7718 - acc: 0.3698\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.7220 - acc: 0.3883\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6760 - acc: 0.4066\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6475 - acc: 0.4169\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6135 - acc: 0.4293\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5903 - acc: 0.4391\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5669 - acc: 0.4474\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5442 - acc: 0.4552\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5207 - acc: 0.4620\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5034 - acc: 0.4687\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4870 - acc: 0.4739\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4683 - acc: 0.4813\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4541 - acc: 0.4841\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4404 - acc: 0.4940\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4264 - acc: 0.4976\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4095 - acc: 0.5021\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3966 - acc: 0.5062\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.3842 - acc: 0.5109\n",
            "50000/50000 [==============================] - 1s 29us/sample - loss: 1.4108 - acc: 0.5023\n",
            "\n",
            "Train accuracy: 50.2%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.4803 - acc: 0.4773\n",
            "\n",
            "Test accuracy: 47.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V34bHfh2pb9k",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 256-128-64, Training Data Accuracy is 50.2% and Test Data Accuracy is 47.7%. Next step is to see if accuracy will improve as we change the hidden units' configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgsb5Cpzpy_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "391afae0-f303-4e32-e98a-6c195fbe6ddc"
      },
      "source": [
        "# Set hidden units to 64-128-256\n",
        "run_model([64,128,256])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 64)                196672    \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 240,586\n",
            "Trainable params: 240,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 2.0919 - acc: 0.2350\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.8969 - acc: 0.3213\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.8274 - acc: 0.3483\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.7746 - acc: 0.3671\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.7327 - acc: 0.3841\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6978 - acc: 0.3958\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6701 - acc: 0.4046\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6464 - acc: 0.4151\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6238 - acc: 0.4249\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6025 - acc: 0.4315\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5808 - acc: 0.4389\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5635 - acc: 0.4464\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5479 - acc: 0.4490\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5313 - acc: 0.4575\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5198 - acc: 0.4612\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5073 - acc: 0.4662\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4943 - acc: 0.4692\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4839 - acc: 0.4751\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4692 - acc: 0.4777\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4616 - acc: 0.4799\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 1.4277 - acc: 0.4977\n",
            "\n",
            "Train accuracy: 49.8%\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 1.4758 - acc: 0.4803\n",
            "\n",
            "Test accuracy: 48.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhBYOY7CqaQz",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 64-128-256, Training Data Accuracy is 49.8% and Test Data Accuracy is 48.0%. Next step is to see if accuracy will improve as we change the hidden units' configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2BvqhvWqYXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbc39539-bd08-4193-eb98-9645110157c8"
      },
      "source": [
        "# Set hidden units to 256-128-256\n",
        "run_model([256,128,256])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 855,178\n",
            "Trainable params: 855,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 2.0344 - acc: 0.2727\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.8484 - acc: 0.3446\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.7695 - acc: 0.3736\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.7169 - acc: 0.3914\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6754 - acc: 0.4057\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6429 - acc: 0.4200\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6141 - acc: 0.4312\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5850 - acc: 0.4419\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5639 - acc: 0.4480\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5448 - acc: 0.4531\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5252 - acc: 0.4620\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5053 - acc: 0.4689\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4878 - acc: 0.4726\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4736 - acc: 0.4779\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4587 - acc: 0.4828\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4439 - acc: 0.4880\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4285 - acc: 0.4958\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4137 - acc: 0.5002\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4034 - acc: 0.5033\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.3897 - acc: 0.5076\n",
            "50000/50000 [==============================] - 1s 30us/sample - loss: 1.3947 - acc: 0.5056\n",
            "\n",
            "Train accuracy: 50.6%\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 1.4567 - acc: 0.4821\n",
            "\n",
            "Test accuracy: 48.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n86ysb1Nqt0W",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 256-128-256, Training Data Accuracy is 50.6% and Test Data Accuracy is 48.2%. Next step is to see if accuracy will improve as we change the hidden units' configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY3R_wwtqyjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dbc969c-0f47-4032-c37f-336750063b1d"
      },
      "source": [
        "# Set hidden units to 256-128-256\n",
        "run_model([128,256,128])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 128)               393344    \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 460,554\n",
            "Trainable params: 460,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 2.0529 - acc: 0.2611\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.8544 - acc: 0.3407\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.7802 - acc: 0.3686\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.7249 - acc: 0.3900\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.6876 - acc: 0.4013\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.6554 - acc: 0.4116\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.6274 - acc: 0.4232\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6004 - acc: 0.4342\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5783 - acc: 0.4414\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5601 - acc: 0.4472\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5401 - acc: 0.4545\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5212 - acc: 0.4623\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5030 - acc: 0.4682\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.4853 - acc: 0.4763\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4709 - acc: 0.4814\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4553 - acc: 0.4852\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.4443 - acc: 0.4882\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4287 - acc: 0.4959\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4164 - acc: 0.5005\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.4035 - acc: 0.5028\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 1.4223 - acc: 0.4893\n",
            "\n",
            "Train accuracy: 48.9%\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 1.4770 - acc: 0.4685\n",
            "\n",
            "Test accuracy: 46.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdVOe1M2rCZ6",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 128-256-128, Training Data Accuracy is 48.9% and Test Data Accuracy is 46.8%. Next step is to see if accuracy will improve as we change the hidden units' configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyYIYV2prbtu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8a322fe-53bf-407a-ba12-aab2712c0fcf"
      },
      "source": [
        "# Set hidden units to 256-256-256\n",
        "run_model([128,128,128])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 128)               393344    \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 427,658\n",
            "Trainable params: 427,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 2.0468 - acc: 0.2589\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.8653 - acc: 0.3337\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 1.7906 - acc: 0.3678\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.7376 - acc: 0.3855\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6973 - acc: 0.4001\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6624 - acc: 0.4128\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6357 - acc: 0.4201\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.6148 - acc: 0.4303\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5913 - acc: 0.4396\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5705 - acc: 0.4463\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5555 - acc: 0.4505\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5346 - acc: 0.4590\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.5237 - acc: 0.4639\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5105 - acc: 0.4666\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4927 - acc: 0.4732\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4776 - acc: 0.4790\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4648 - acc: 0.4795\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4535 - acc: 0.4861\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4373 - acc: 0.4921\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.4283 - acc: 0.4939\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.3994 - acc: 0.5076\n",
            "\n",
            "Train accuracy: 50.8%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.4564 - acc: 0.4842\n",
            "\n",
            "Test accuracy: 48.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cAU0RlrlSw",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 128-128-128, Training Data Accuracy is 50.8% and Test Data Accuracy is 48.4%. Next step is to see if accuracy will improve as we change the hidden units' configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MArKqLIgrRrR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a7a0c66-9adb-4686-f1fb-7d40fa000abe"
      },
      "source": [
        "# Set hidden units to 256-256-256\n",
        "run_model([256,256,256])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 920,842\n",
            "Trainable params: 920,842\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 2.0387 - acc: 0.2682\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.8489 - acc: 0.3457\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.7746 - acc: 0.3737\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.7197 - acc: 0.3930\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6735 - acc: 0.4088\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6371 - acc: 0.4227\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.6104 - acc: 0.4336\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5832 - acc: 0.4401\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5570 - acc: 0.4486\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5372 - acc: 0.4582\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5194 - acc: 0.4631\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5016 - acc: 0.4704\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4815 - acc: 0.4778\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4646 - acc: 0.4827\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4474 - acc: 0.4892\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4390 - acc: 0.4929\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4167 - acc: 0.4998\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4061 - acc: 0.5024\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3909 - acc: 0.5079\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3750 - acc: 0.5119\n",
            "50000/50000 [==============================] - 2s 30us/sample - loss: 1.3998 - acc: 0.4962\n",
            "\n",
            "Train accuracy: 49.6%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.4738 - acc: 0.4669\n",
            "\n",
            "Test accuracy: 46.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af-Hb5A6q_Xv",
        "colab_type": "text"
      },
      "source": [
        "With hidden units set to 256-256-256, Training Data Accuracy is 49.6% and Test Data Accuracy is 46.7%. Among the various configurations that were tested, the 128-128-128 configuration generated the highest Training Data Accuracy (50.8%) and Test Data Accuracy (48.4%). Thus, we use the 128-128-128 configuration for succeeding tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcjBxOSUq35W",
        "colab_type": "text"
      },
      "source": [
        "After hyperparameter tuning, we now train the MLP over **200 epochs**. For every 20th epoch, Training & Test Data Accuracies are generated for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja4lFWNKB1jC",
        "colab_type": "code",
        "outputId": "3c8610c4-7a3b-4dca-d885-60f99e306be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set chosen hyperparameter values\n",
        "hidden_units = [128,128,128]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units[0], input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units[1]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units[2]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "  \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the MLP\n",
        "i = 1\n",
        "epoch = 20\n",
        "epoch_num = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "while i <= 10:\n",
        "  epoch_num.append(epoch * i)\n",
        "  model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
        "  train_score = model.evaluate(x_train, y_train, batch_size=128)\n",
        "  print(\"\\nTrain accuracy: %.1f%%\" % (100.0 * train_score[1]))\n",
        "  train_acc.append(train_score[1])\n",
        "  test_score = model.evaluate(x_test, y_test, batch_size=128)\n",
        "  print(\"\\nTest accuracy: %.1f%%\" % (100.0 * test_score[1]))\n",
        "  test_acc.append(test_score[1])\n",
        "  i = i + 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_40 (Dense)             (None, 128)               393344    \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 427,658\n",
            "Trainable params: 427,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 2.0345 - acc: 0.2634\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.8520 - acc: 0.3412\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.7828 - acc: 0.3684\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.7277 - acc: 0.3875\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6903 - acc: 0.4015\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.6548 - acc: 0.4136\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.6341 - acc: 0.4210\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.6051 - acc: 0.4340\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.5834 - acc: 0.4396\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.5623 - acc: 0.4482\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5445 - acc: 0.4542\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5241 - acc: 0.4624\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.5064 - acc: 0.4682\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4921 - acc: 0.4737\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4780 - acc: 0.4777\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4653 - acc: 0.4816\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4491 - acc: 0.4873\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.4351 - acc: 0.4921\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4238 - acc: 0.4967\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4182 - acc: 0.4992\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.4038 - acc: 0.5048\n",
            "\n",
            "Train accuracy: 50.5%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.4579 - acc: 0.4870\n",
            "\n",
            "Test accuracy: 48.7%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.4038 - acc: 0.5043\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3896 - acc: 0.5079\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3853 - acc: 0.5095\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3710 - acc: 0.5181\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3635 - acc: 0.5177\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 1.3515 - acc: 0.5217\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3498 - acc: 0.5230\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3372 - acc: 0.5290\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3301 - acc: 0.5293\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3228 - acc: 0.5328\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3148 - acc: 0.5364\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.3074 - acc: 0.5393\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2972 - acc: 0.5426\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2918 - acc: 0.5431\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.2858 - acc: 0.5472\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2764 - acc: 0.5494\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.2706 - acc: 0.5537\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2612 - acc: 0.5549\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2574 - acc: 0.5559\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2513 - acc: 0.5599\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 1.2265 - acc: 0.5660\n",
            "\n",
            "Train accuracy: 56.6%\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 1.3765 - acc: 0.5095\n",
            "\n",
            "Test accuracy: 51.0%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2433 - acc: 0.5623\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2379 - acc: 0.5613\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2303 - acc: 0.5639\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2231 - acc: 0.5674\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2203 - acc: 0.5694\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2130 - acc: 0.5720\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2074 - acc: 0.5747\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.2018 - acc: 0.5759\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1925 - acc: 0.5785\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1902 - acc: 0.5783\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1851 - acc: 0.5814\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1800 - acc: 0.5837\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1716 - acc: 0.5845\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1682 - acc: 0.5891\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1609 - acc: 0.5894\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1562 - acc: 0.5917\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1510 - acc: 0.5929\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1409 - acc: 0.5997\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1395 - acc: 0.5977\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1332 - acc: 0.6007\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 1.1235 - acc: 0.6040\n",
            "\n",
            "Train accuracy: 60.4%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.3735 - acc: 0.5171\n",
            "\n",
            "Test accuracy: 51.7%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1300 - acc: 0.6001\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1258 - acc: 0.6026\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1175 - acc: 0.6077\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1127 - acc: 0.6057\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.1089 - acc: 0.6089\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.1055 - acc: 0.6104\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0999 - acc: 0.6139\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0982 - acc: 0.6119\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0909 - acc: 0.6137\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.0829 - acc: 0.6161\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0839 - acc: 0.6185\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0751 - acc: 0.6193\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0721 - acc: 0.6186\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0649 - acc: 0.6249\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 1.0603 - acc: 0.6259\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0585 - acc: 0.6263\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0501 - acc: 0.6291\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0505 - acc: 0.6282\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0449 - acc: 0.6313\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0397 - acc: 0.6333\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.0137 - acc: 0.6431\n",
            "\n",
            "Train accuracy: 64.3%\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 1.3525 - acc: 0.5289\n",
            "\n",
            "Test accuracy: 52.9%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0371 - acc: 0.6333\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0302 - acc: 0.6353\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0231 - acc: 0.6386\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0205 - acc: 0.6381\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0141 - acc: 0.6415\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0173 - acc: 0.6408\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0032 - acc: 0.6455\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 1.0040 - acc: 0.6459\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9995 - acc: 0.6459\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9913 - acc: 0.6486\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9923 - acc: 0.6503\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9812 - acc: 0.6526\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9788 - acc: 0.6541\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.9792 - acc: 0.6559\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.9727 - acc: 0.6565\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.9710 - acc: 0.6579\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9694 - acc: 0.6583\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.9634 - acc: 0.6614\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 33us/sample - loss: 0.9540 - acc: 0.6639\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9508 - acc: 0.6651\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 0.9435 - acc: 0.6676\n",
            "\n",
            "Train accuracy: 66.8%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.4126 - acc: 0.5242\n",
            "\n",
            "Test accuracy: 52.4%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9494 - acc: 0.6674\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9463 - acc: 0.6663\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9420 - acc: 0.6702\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9339 - acc: 0.6713\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9358 - acc: 0.6702\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9317 - acc: 0.6717\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9280 - acc: 0.6707\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9220 - acc: 0.6737\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9112 - acc: 0.6786\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9130 - acc: 0.6791\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.9063 - acc: 0.6817\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.9043 - acc: 0.6830\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8979 - acc: 0.6831\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8933 - acc: 0.6848\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8945 - acc: 0.6853\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8923 - acc: 0.6865\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8820 - acc: 0.6924\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8801 - acc: 0.6903\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8713 - acc: 0.6943\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8730 - acc: 0.6942\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 1.0304 - acc: 0.6224\n",
            "\n",
            "Train accuracy: 62.2%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.6017 - acc: 0.4876\n",
            "\n",
            "Test accuracy: 48.8%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8707 - acc: 0.6922\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8633 - acc: 0.6962\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8621 - acc: 0.6963\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8526 - acc: 0.7006\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8532 - acc: 0.7008\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8501 - acc: 0.6999\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8414 - acc: 0.7032\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8469 - acc: 0.7012\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8360 - acc: 0.7047\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8329 - acc: 0.7056\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.8306 - acc: 0.7071\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8278 - acc: 0.7085\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8206 - acc: 0.7116\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8176 - acc: 0.7136\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8152 - acc: 0.7111\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8097 - acc: 0.7142\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8119 - acc: 0.7130\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.8036 - acc: 0.7196\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.8028 - acc: 0.7155\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.7965 - acc: 0.7204\n",
            "50000/50000 [==============================] - 1s 30us/sample - loss: 0.8398 - acc: 0.7038\n",
            "\n",
            "Train accuracy: 70.4%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.5598 - acc: 0.5133\n",
            "\n",
            "Test accuracy: 51.3%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7926 - acc: 0.7199\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7888 - acc: 0.7223\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7880 - acc: 0.7233\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7877 - acc: 0.7225\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7792 - acc: 0.7225\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7703 - acc: 0.7275\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7643 - acc: 0.7317\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7633 - acc: 0.7304\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7605 - acc: 0.7329\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7566 - acc: 0.7345\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7524 - acc: 0.7352\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7592 - acc: 0.7316\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7459 - acc: 0.7373\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7494 - acc: 0.7362\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7425 - acc: 0.7380\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7379 - acc: 0.7417\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7408 - acc: 0.7385\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7325 - acc: 0.7421\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7243 - acc: 0.7453\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7246 - acc: 0.7440\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 0.6846 - acc: 0.7605\n",
            "\n",
            "Train accuracy: 76.1%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.5730 - acc: 0.5190\n",
            "\n",
            "Test accuracy: 51.9%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7206 - acc: 0.7459\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7108 - acc: 0.7494\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7173 - acc: 0.7467\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7177 - acc: 0.7460\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7078 - acc: 0.7510\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7017 - acc: 0.7549\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.7011 - acc: 0.7511\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.7036 - acc: 0.7529\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6888 - acc: 0.7573\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6958 - acc: 0.7542\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6866 - acc: 0.7591\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6848 - acc: 0.7580\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6836 - acc: 0.7582\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6797 - acc: 0.7613\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6773 - acc: 0.7613\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6745 - acc: 0.7629\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6696 - acc: 0.7626\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6701 - acc: 0.7634\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6626 - acc: 0.7661\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6593 - acc: 0.7683\n",
            "50000/50000 [==============================] - 1s 27us/sample - loss: 0.6510 - acc: 0.7723\n",
            "\n",
            "Train accuracy: 77.2%\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 1.6989 - acc: 0.5102\n",
            "\n",
            "Test accuracy: 51.0%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6640 - acc: 0.7660\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6529 - acc: 0.7703\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6455 - acc: 0.7733\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6416 - acc: 0.7725\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6447 - acc: 0.7749\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6405 - acc: 0.7744\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6451 - acc: 0.7728\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6333 - acc: 0.7787\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6225 - acc: 0.7815\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6304 - acc: 0.7780\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6200 - acc: 0.7813\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6320 - acc: 0.7769\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6228 - acc: 0.7819\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6266 - acc: 0.7799\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6115 - acc: 0.7851\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6090 - acc: 0.7868\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6062 - acc: 0.7880\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.5976 - acc: 0.7900\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 35us/sample - loss: 0.6092 - acc: 0.7866\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 34us/sample - loss: 0.6005 - acc: 0.7901\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 0.6920 - acc: 0.7494\n",
            "\n",
            "Train accuracy: 74.9%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.8852 - acc: 0.5007\n",
            "\n",
            "Test accuracy: 50.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyhu42syrO3U",
        "colab_type": "text"
      },
      "source": [
        "The model's Training and Test Data Accuracy is plotted against the number of epochs performed during training. Looking at the plot, it can be observed that **as the number of epochs increase, Training Data Accuracy increases**. On the other hand, **Test Data Accuracy gradually decreases after 80 epochs**, indicating **model overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rtt3DxnsPms",
        "colab_type": "code",
        "outputId": "c814ba64-eb46-44aa-bef6-6902e5c278f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Generate plot showing Training & Test Data Accuracy\n",
        "plt.plot(epoch_num, train_acc)\n",
        "plt.plot(epoch_num, test_acc)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VPW5x/HPQ0IIOwk7JCQBkX0P\nYXPB9qq4gYoL4IYKqNXW1larV7tc7bVotWqVimhVQAXXWtpbF1DRimwJOwHCDgkQlkAgZM88948z\nkUlMYIBJzmTmeb9e88rMmXNmnpxMvuec3+93zoiqYowxJjzUc7sAY4wxtcdC3xhjwoiFvjHGhBEL\nfWOMCSMW+sYYE0Ys9I0xJoxY6BtjTBix0DfGmDBioW+MMWEk0u0CKmvVqpUmJia6XYYxxtQpaWlp\nB1W19anmC7rQT0xMJDU11e0yjDGmThGRnf7MZ807xhgTRiz0jTEmjFjoG2NMGLHQN8aYMGKhb4wx\nYcRC3xhjwoiFvjHGhJGgG6dvjDG1JetIAcu355BfXMbo/h1o0iD0IzH0f0NjjAFUle0Hj7Nsew7L\ntuewdHsOWUcKvn9+6icbuG14IhOHJ9KySQMXK61ZFvrGmJBU5lE27TvGsu2HWLbDCfqDecUAtGoS\nRUpSLJPPT2JwUizFpR5e+XobL321hVf/s40bk+OZdH5n4mMbufxbBJ6oqts1VJCcnKx2GQZjzOkq\nLvWwbk/u93vyy3fkcKywFICOLRoyJCmWlKRYBifF0rlVY0TkB6+xZX8eM77Zyt9XZuFRGN2vA3df\n2IVu7ZrW9q9z2kQkTVWTTzmfhb4xpi4qKC5j5e7D34f8il2HKSzxANCldWNSklqSkhTD4MRY4mJO\nb499b24Bf/vPdt5Ztov84jL+q0cb7hnZhUEJsTXxqwSEhb4xJqQcLSwhbcdhlm7PYdn2Q6zNyqWk\nTBGBnu2bkZIUS0qisyffKkBt8oePFzNr8U7e/G47h/NLSEmM5Z6RXRjZrXWVRwpustA3xtRpB/OK\nWO7tcF22PYcN+46iCvUjhL5xLRicGMuQpFgGJsTQvGH9Gq0lv7iUd5fv5tVvtrEnt5Du7Zpyz8gu\nXNGnPZERwTHy3ULfGFOnZB0pcDpdvUG/7cBxAKLr12NgpxhnTz4plgHxMTSMinClxpIyD/NW7WH6\n11vZvD+PuJiG3HVBZ65Pjie6vjs1lbPQN8YEtaLSMj5emcWSbc6efPnwyabRkQxOjP0+5Ht3aE5U\nZHDsTZfzeJQFG7L568KtrNp9hFZNorh9RBI3D02o8aOO6ljoG2OClsej3PvOCj5Zt49WTRqQkhRD\nSmIsKUkt6dauKRH1gqu9vDqqytLtOby8cCtfZxygSYNIbhraiTtHJNGmWXSt1uJv6Ns4fWNMrXvq\n0418sm4fj17eg0nnJwVdp6i/RIShnVsytHNL1u/JZfrX23j1m2288e0Oxg6K464LOpPYqrHbZVZg\ne/rGmFr19tKdPPr3ddw6LIH/Gd2rzgZ+dXYeOs6Mb7bxflompWUeLuvTnnsu7ELvjs1r9H2teccY\nE3S+zjjAHW8u54KurXj11uSgGflSE/YfK+SNRTt4a/FOjhWVcn7XVtwzsgvDOreskQ1dQENfREYB\nLwARwGuqOrXS888BF3kfNgLaqGoL73NlwFrvc7tUdfTJ3stC35jQtGHvUa6fvphOsY14/+5hNA6D\ni5uBc37BW0t28vq32zmYV0y/+Bbcc2EXLunZlnoB7LsIWOiLSASQAVwMZALLgfGqml7N/D8FBqjq\nHd7HearaxN/CLfSNCT3ZRwu5etoiVOHje0fQrnntdnIGg8KSMj5Iy2TGN9vYlZNPl9aNufvCLozp\n3zEgo5P8DX1/3ikF2KKq21S1GJgLjDnJ/OOBOf6VaYwJdceLSrlz5nKOFpTw+sTBYRn4ANH1I7h5\naAJf/vJC/jJ+AFGRETz4wRou/NNX/O3b7RwvKq2VOvwJ/Y7Abp/Hmd5pPyAiCUAS8KXP5GgRSRWR\nJSJy9RlXaoypc8o8yv1zV5K+5ygvTRhIzw7N3C7JdZER9RjdrwP//tl5vHn7YOJjG/HEv9IZ8dSX\nvLBgMzXdzxroRrVxwAeqWuYzLUFVs0SkM/CliKxV1a2+C4nIFGAKQKdOnQJckjHGLU/8K50FG/bz\nxJheXNS9jdvlBBURYWS3Nozs1oa0nTm8vHAbG/cdrfHRTP6EfhYQ7/M4zjutKuOAe30nqGqW9+c2\nEVkIDAC2VppnBjADnDZ9fwo3xgS3NxZt583vdjDpvCRuGZbodjlBbVBCLK/dFktpmafG38uf5p3l\nQFcRSRKRKJxgn1d5JhHpDsQAi32mxYhIA+/9VsAIoMoOYGNM6FiQns0T/0rn0l5teeTyHm6XU2fU\nxhDWU+7pq2qpiNwHfIYzZPN1VV0vIo8DqapavgEYB8zVig1SPYBXRMSDs4GZWt2oH2NMaFibmctP\n56ykT8fmPH/jgDpzSYVwYSdnGWMCZs+RAq6etoj6EfX4+73DadM0PEfquCGQQzaNMeaUjhWWcMeb\nyykoLuON2wdb4Aep8DglzhhTo0rKPNz7zkq27M/jzdtTOLdt8H+nbLiy0DfGnBVV5Xfz1vNNxgGe\nGtuH87q2crskcxLWvGOMOSszvtnGO0t38ZORXbhxsJ1nE+ws9I0xZ+zfa/fyx082cmXf9vzqkm5u\nl2P8YKFvjDkjK3Yd5hfvrmJQQgzPXN8voFeMNDXHQt8Yc9p25+QzeWYqbZtFM+OWQa5/Kbjxn4W+\nMea05OaXMPGNZZR6lDduH0zLJg3cLsmcBgt9Y4zfiks93P1WGrty8nnllkF0ae33V2WYIGFDNo0x\nflFVHvloLYu3HeK5G/sxtHNLt0syZ8D29I0xfnnpyy18uCKTn/9XV64ZEOd2OeYMWegbY07pH6uy\neHZ+BtcO6Mj9P+7qdjnmLFjoG2NOatn2HB58fw1DkmL549g+Nf4lH6ZmWegbY6q1/eBxpsxOJS62\nIa/cMogGkTY0s66z0DfGVCnneDG3v7GMeiK8MXEwLRpFuV2SCQAbvWOM+YHCkjKmzEplT24hcyYP\nIaFlY7dLMgFie/rGmAo8HuWhD9aQuvMwf76hH4MSYt0uyQSQhb4xpoLnFmQwb/UeHhrVjSv7dnC7\nHBNgFvrGmO+9l7qbF7/cwrjB8dxzYRe3yzE1wELfGAPAd1sO8t8freX8rq144ureNjQzRFnoG2PY\nnH2Mu95Ko3Prxky7aSD1IywaQpX9ZY0JcweOFXH7m8tpEBnB6xMH0yy6vtslmRpkoW9MGCsoLmPS\nrFQO5hXxt9uSiYtp5HZJpobZOH1jwpTHo/zi3VWsyTzC9JsH0S++hdslmVpge/rGhKmpn27k0/X7\nePTyHlzaq53b5ZhaYqFvTBh6a8lOZnyzjVuHJXDneUlul2NqkV+hLyKjRGSTiGwRkYereP45EVnl\nvWWIyBGf524Tkc3e222BLN4Yc/rWZB7hd/PWc1G31vz2yp42NDPMnLJNX0QigGnAxUAmsFxE5qlq\nevk8qvoLn/l/Cgzw3o8FfgckAwqkeZc9HNDfwhjjF49H+c0/1hPbOIoXxg8g0oZmhh1//uIpwBZV\n3aaqxcBcYMxJ5h8PzPHevxSYr6o53qCfD4w6m4KNMWfuvdTdrN59hP++vLsNzQxT/oR+R2C3z+NM\n77QfEJEEIAn48nSWFZEpIpIqIqkHDhzwp25jzGk6kl/MU59uJCUxlqv7V/kvbMJAoI/txgEfqGrZ\n6SykqjNUNVlVk1u3bh3gkowxAH/6bBNHC0t5/Ope1o4fxvwJ/Swg3udxnHdaVcZxomnndJc1xtSQ\nNZlHeGfZLm4blkj3ds3cLse4yJ/QXw50FZEkEYnCCfZ5lWcSke5ADLDYZ/JnwCUiEiMiMcAl3mnG\nmFpS3nnbsnEDfn6xfal5uDvl6B1VLRWR+3DCOgJ4XVXXi8jjQKqqlm8AxgFzVVV9ls0RkSdwNhwA\nj6tqTmB/BWPMyZR33j53Yz/rvDWIT0YHheTkZE1NTXW7DGNCwpH8Yi56ZiFd2zTl3buGWlt+CBOR\nNFVNPtV8NkjXmBD2zOdO5+3/jLHOW+Ow0DcmRK3NzOXtpbu4dVgCPdpb561xWOgbE4Kcztt1tGzc\ngF9cfK7b5ZggYqFvTAh6P203q+zMW1MFC31jQoxz5u0mBifGcM0AO/PWVGShb0yIeebzTeQWlPD4\nGPtyc/NDFvrGhJDyzttbhlrnramahb4xIeJE522Udd6aalnoGxMiPkjLZNXuIzxyWQ+aN7TOW1M1\nC31jQkBufglTP91IckIM1w60zltTPQt9Y0LAM59v4kh+sXXemlOy0DemjluXlcvbS3dy67BEenaw\nzltzchb6xtRh5Z23sdZ5a/xkoW9MHfbBikxW7jrCw9Z5a/xkoW9MHZWbX8LUTzYyKCGGa+3MW+Mn\nC31j6qhn55d33vaiXj3rvDX+sdA3Ic3jUT5dt5erXvyWe99ZQWFJmdslBcS6rFzeWrKTW4Ym0KtD\nc7fLMXXIKb8u0Zi6yONRPk/P5oUvNrNh71E6tmjIuj25HMor4tVbk2lah6886fEov/3HOmIaRfHA\nJd3cLsfUMbanb0KKqvLZ+n1c+eK33P1WGoUlZTx3Yz++fnAkz9/Yn9Qdh5nw6lIO5RW5XeoZ+3BF\nJit2HeHhy7pb5605bbanb0KCqjI/PZvnF2wmfe9REls24s839GN0vw5ERjj7NmP6d6RZdH3ueTuN\n66cvZvakIXRs0dDlyk+Pb+ft2IFxbpdj6iDb0zd1WnnYX/nit0yZnUZ+cSnPXt+PBQ9cyLUD474P\n/HIXdW/D7DuHcCCviOte/o4t+/NcqvzM/Hn+Jg5b5605Cxb6pk5SVRakZ3PVS98yeVYqeUWlPOMN\n+7GDfhj2vgYnxvLulGGUlCnXT/+ONZlHarHyM7d+Ty6zl+zkZuu8NWfBQt/UKarKFxuyGf3SIibN\nSuVoQSl/uq4vXzxwIdedIux99ezQjA/uHkbjBpGMn7GE77YerOHKz47TebuemEZR/PJi67w1Z85C\n39QJqsqXG7MZM20Rd85M5UhBMU9f15cvfnkh1yfH+x32vhJbNebDe4bTMaYhE19fzmfr99VA5YHx\n0cos0nYe5teXdad5I+u8NWfOOnJNUFNVFm46wPMLMlidmUt8bEOeHtuXawZ2pP4ZBH1lbZtF895d\nw7j9zeXc81YaU8f25Ybk+ABUHji5BSX88d8bGNipBddZ5605S36FvoiMAl4AIoDXVHVqFfPcAPwe\nUGC1qk7wTi8D1npn26WqowNQtwlxqsrCjAM8v2Azq3cfIS6mIU+N7cO1A+MCEva+WjSK4u1JQ7hr\ndhoPfbCGowUlTDq/c0Df42w8Nz+Dw/nFzLwjxTpvzVk7ZeiLSAQwDbgYyASWi8g8VU33macr8Agw\nQlUPi0gbn5coUNX+Aa7bhChV5Wtv2K/afYSOLRoy9Von7KMia641slFUJK/dlswD767mD/+3gZzj\nxTx4aTfXr02/fk8usxbv4OahCfTuaJ235uz5s6efAmxR1W0AIjIXGAOk+8wzGZimqocBVHV/oAs1\noU1V+WbzQZ5fkMHKXU7Y//HaPoyt4bD31SAygr+MH0CzhvX568KtHCko4YkxvYlwae/a41F+Z523\nJsD8Cf2OwG6fx5nAkErznAsgIotwmoB+r6qfep+LFpFUoBSYqqofn13JJpSoKv/xhv0Kb9g/eU0f\nrhtUe2HvK6Ke8OQ1vYlp5AR/bkEJz93Q35VaPlqZRerOwzw9tq913pqACVRHbiTQFRgJxAHfiEgf\nVT0CJKhqloh0Br4UkbWqutV3YRGZAkwB6NSpU4BKMsFMVfl2y0GeX7CZtJ2H6dA8mv+9pjfXD4p3\nJWB9iQgPjepOTKMo/vffGzhaUMIrtwyiUVTtjXvILShh6icbGNCpBdcNss5bEzj+fIqzAN/hDHHe\nab4ygaWqWgJsF5EMnI3AclXNAlDVbSKyEBgAVAh9VZ0BzABITk7WM/g9TB2hqizacojnF2SQuvMw\n7ZtH88TVvbkhOY4GkRFul1fB5As607xRfR7+cA03vbaUNyYOpkWjqFp57+fmZ3DoeDFv3m6dtyaw\n/An95UBXEUnCCftxwIRK83wMjAfeEJFWOM0920QkBshX1SLv9BHA0wGr3tQZqsp3W52wX77jMO2a\nRfPEmF7cMDg+6MLe1w3J8TSLrs/P5qzkxleWMOvOFNo2i67R90zfc9TpvB1inbcm8E4Z+qpaKiL3\nAZ/htNe/rqrrReRxIFVV53mfu0RE0oEy4EFVPSQiw4FXRMSDcyLYVN9RPyb0qSqLtx7i+QWbWbYj\nh3bNonl8TC9uDPKw9zWqdzvevH0wk2elct3075h9xxASWzWukfdSdS6b3KJRFL+yyyabGiCqwdWa\nkpycrKmpqW6XYQLgWGEJv3p/NZ+tz6Ztswb8ZOQ53Dg4nuj6dSPsK1u9+wgT31hGRL16zLojhZ4d\nmgX8PT5My+SX76/m6bF9uWFwcJ0kZoKbiKSpavKp5rPLMJgasfVAHldPW8SCDft5aFQ3vn7wIm4b\nnlhnAx+gX3wL3r97OPUjhBtnLCZ1R05AXz+3oIQ/frKB/vHWeWtqjoW+CbgF6dlc/dIiDueXMPvO\nFH4y8pw6Hfa+zmnThA/uGU7rJg24+W9L+Wpj4E5JKe+8fWJMb+u8NTXGQt8EjMejvLBgM5NmpZLQ\nqhHz7hvB8C6t3C4r4Dq2aMj7dw/jnDZNmDwrlX+sqjyY7fSVd97eNKQTfeKs89bUHAt9ExDHCku4\n+600nluQwTUDOvLB3cOJi2nkdlk1pmWTBsyZPJRBCTH8/N1VzF6844xfS1X53bx1NG9Y3zpvTY2z\n0Ddnrbz9/ouN+/nNlT358w39QqY552SaRtdn5h0p/Lh7W37zj/X85YvNnMnAiL+vzGL5jsM8fFn3\nWjsPwIQvC31zVr7YULH9/s7zkly/SFltiq4fwfSbBzJ2YBx/np/B4/9Kx+PxP/iPFpbw5L830j++\nBdcPstE6pubZ9fTNGfF4lJe+2sKf52fQq0MzXrllUEg355xMZEQ9/nRdX5o3rM/ri7aTm1/CU9f1\n9esS0E7nbRFvTBxsnbemVljom9OWV1TKA++u4vP0bK4Z0JE/XtsnLJpzTqZePeE3V/YgtnF9nvk8\ng6OFJbw0YeBJ18uGvUeZtXgnE1Ks89bUHmveMadlm0/7/WNX9Aib9nt/iAj3/agrT1zdmy827ufW\n15dxtLCkynnLz7xtFh3Jg5da562pPRb6xm9fbsxmzEuLOJRXxOw7Uph0fuewar/31y1DE3hh3ABW\n7DzM+BlLOJhX9IN5Pl7ldN7+epR13praZaFvTsnjUV78YjN3zkylU8tG/POn5zH8nNAbfx9Io/t1\n4LXbktl6II/rpy8m83D+988dLSzhf/9vI/3iWwTd9/Ga0Gehb04qr6iUe95O49n5GYzp1yHkx98H\n0shubXh70hAO5RVx3cuL2Zx9DIDn52/m0PEinhjTyzpvTa2z0DfVKm+/n5+ezWNX9OC5G/vTMMra\n70/HoIRY3r1rGGWq3PDKYt5L3c3MxTsYn9KJvnEt3C7PhCELfVOlLzdmM2aa037/1p1DrP3+LPRo\n34wP7h5G0+j6PPTBGqfz1s68NS6xIZumAlVl2ldbeHZ+Bj3aOePv42OtOedsJbRszAd3D+Phj9Zy\n/aA4Yhpb561xh4W++V5eUSm/em81n67fx+h+HXhqbF9rzgmgNs2ieX3iYLfLMGHOQt8AsP3gcabM\nSmXrgTweu6JH2F1OwZhwYaFv+GrTfn42ZyWR9YTZdw5hhA3HNCZkWeiHMVXlrwu38sznm+jerhkz\nrP3emJBnoR+mrP3emPBkoR+Gdhw8zmRv+/2jl/dg0vnWfm9MuLDQDzNfbdrP/XNWUq+eMOuOIZzX\n1drvjQknFvphwtrvjTFgoR8WjheV8qv3V/PJun1c1a8DT43tQ6Mo+9MbE47sPz/E7Th4nLtmp7F5\n/zH++/LuTLbLKRgT1iz0Q9hC7/j7evWEmXekcH7X1m6XZIxxmV8XXBORUSKySUS2iMjD1cxzg4ik\ni8h6EXnHZ/ptIrLZe7stUIWb6nk8zvVzbn9zOR1aNOSf951ngW+MAfzY0xeRCGAacDGQCSwXkXmq\nmu4zT1fgEWCEqh4WkTbe6bHA74BkQIE077KHA/+rGIDcghJ++d4qFmzYb+33xpgf8CcNUoAtqroN\nQETmAmOAdJ95JgPTysNcVfd7p18KzFfVHO+y84FRwJzAlG98pe85yj1vp5F1uIDfXdWTicMTrf3e\nGFOBP6HfEdjt8zgTGFJpnnMBRGQREAH8XlU/rWbZjmdcranWh2mZPPrxWpo3rM/cKUNJTox1uyRj\nTBAK1HF/JNAVGAnEAd+ISB9/FxaRKcAUgE6dOgWopPBQVFrGE/9K560luxiSFMtLEwbSumkDt8sy\nxgQpfzpyswDfb2+O807zlQnMU9USVd0OZOBsBPxZFlWdoarJqprcurV1OPprz5ECbnxlCW8t2cVd\nF3Tm7UlDLPCNMSflT+gvB7qKSJKIRAHjgHmV5vkYZy8fEWmF09yzDfgMuEREYkQkBrjEO82cpUVb\nDnLli9+yOfsYL980kEcu70FkhH37pTHm5E7ZvKOqpSJyH05YRwCvq+p6EXkcSFXVeZwI93SgDHhQ\nVQ8BiMgTOBsOgMfLO3XNmfF4lJe/3sqzn2+ic+smTL95EOe0aeJ2WcaYOkJU1e0aKkhOTtbU1FS3\nywhKRwtL+OV7q5mfns2Vfdvz1Ni+NG5gwzGNMSAiaaqafKr5LDHqiI37jnL37DQyDxfw2yt7cvsI\nG45pjDl9Fvp1wMcrs3j4ozU0i67PnClDGWzDMY0xZ8hCP4gVl3r4w/+lM2vxTlKSYnlpwgDaNI12\nuyxjTB1moR+k9uYW8JO3V7By1xEmn5/EQ6O6U99G5xhjzpKFfhD6butBfvrOSgpLypg2YSBX9G3v\ndknGmBBhoR9EVJVXvtnG059u9A7HHMg5bZq6XZYxJoRY6AeJo4UlPPj+aj5bn80Vfdrz1HV9aWLD\nMY0xAWapEgQ27TvG3W+lsSsnn8eu6MGd5yXZcExjTI2w0HfZP1Zl8fCHa2kSHcmcyUNJSbLhmMaY\nmmOh75LiUg9P/nsDb363g8GJMUybMJA2zWw4pjGmZlnou2BfbiH3vrOCtJ2HufO8JB6+zIZjGmNq\nh4V+LVu89RA/nbOC/OIyXpowgCv7dnC7JGNMGLHQryWqyqv/2cZTn24isWUj5kweSte2NhzTGFO7\nLPRrwbHCEh76YA2frNvH5X3a8fR1/Ww4pjHGFZY8NSwj2xmOufOQDcc0xrjPQr8G/XP1Hn794Roa\nRUXyzqQhDOnc0u2SjDFhzkK/BpSUOcMx31i0g+SEGKbdNJC2NhzTGBMELPQD7Eh+MZNnpbJ8x2Fu\nH5HIf1/ew4ZjGmOChoV+AJV5lJ/OWcnq3bn8ZfwARvez4ZjGmOBioR9Af/psE//ZfJCp1/axwDfG\nBCVrdwiQf63Zw/SvtzJhSCfGpXRyuxxjjKmShX4AbNx3lAffX8OghBh+f1Uvt8sxxphqWeifpSP5\nxUyZlUbT6EhevmkgUZG2So0xwcva9M9CmUe5f+4q9uYWMHfKULtKpjEm6Fnon4VnP9/E1xkHePKa\nPgxKsOvgG2OCn7VFnKF/r93LXxduZXxKPBOGWMetMaZu8Cv0RWSUiGwSkS0i8nAVz08UkQMissp7\nm+TzXJnP9HmBLN4tGdnH+NX7q+kf34Lfj7aOW2NM3XHK5h0RiQCmARcDmcByEZmnqumVZn1XVe+r\n4iUKVLX/2ZcaHHILSpgyK5XGDSKZfvMgGkRGuF2SMcb4zZ89/RRgi6puU9ViYC4wpmbLCk5lHuXn\nc1eSebiAl28aSLvm1nFrjKlb/An9jsBun8eZ3mmVjRWRNSLygYjE+0yPFpFUEVkiIldX9QYiMsU7\nT+qBAwf8r76WPb8gg682HeB3o3uRnGgdt8aYuidQHbn/BBJVtS8wH5jp81yCqiYDE4DnRaRL5YVV\ndYaqJqtqcuvWrQNUUmB9um4fL365hRuS47jZOm6NMXWUP6GfBfjuucd5p31PVQ+papH34WvAIJ/n\nsrw/twELgQFnUa8rNmcf45fvraJffAseH9PbvgTFGFNn+RP6y4GuIpIkIlHAOKDCKBwRae/zcDSw\nwTs9RkQaeO+3AkYAlTuAg9rRwhKmzE6jYVQE028eSHR967g1xtRdpxy9o6qlInIf8BkQAbyuqutF\n5HEgVVXnAT8TkdFAKZADTPQu3gN4RUQ8OBuYqVWM+glaHo/yi7mr2J2TzzuTh9K+eUO3SzLGmLMi\nqup2DRUkJydramqq22UA8Nz8DF74YjOPj+nFrcMS3S7HGGOqJSJp3v7Tk7Izcqvx+fp9vPDFZq4b\nFMctQxPcLscYYwLCQr8KW/bn8cB7q+kb15w/XG0dt8aY0GGhX8mxwhKmzE6lQWQ9pt88yDpujTEh\nxa6y6cPjUR54bzU7D+Xz9qQhdGhhHbfGmNBie/o+XvxyC/PTs3nsih4M7dzS7XKMMSbgLPS9vtiQ\nzXMLMrh2YEcmDk90uxxjjKkRFvrAtgN5/HzuKnp3bMaT1/SxjltjTMgK+9A/5j3jtr513BpjwkBY\nd+R6PMov31vN9oPHmX1nCnExjdwuKTQU50NWKuz8DnYugrz90KQNNGkHTdtCk7aV7reF6OZgR1jG\n1LiwDv2/LtzC5+nZ/ObKngzv0srtcuquwlzYtdQJ+J3fwZ6V4CkBBNr1gZbnwPEDsHsp5GVDaeEP\nXyMy+sSGoUkbaNqu0n3vxqFxa4gI64+tMWclbP97vtq4n2fnZ3B1/w7cMSLR7XLqluMHvXvx3j35\n7HWgHqgXCR0GwrB7IWEExKdAwxYVl1WFoqNwLBvy9jlHAcf2Vbx/aIvzugWHq3hzgcatfnik0LTd\nDzcaUY1rZXUYU5eEZehvP3icn81dSY92zfjjtX2t4/ZUcjNPBPzOxXBwkzM9siHED4YLfw2dhkHc\nYIg6RROZiNOUE90cWp978nmIoIVgAAAPY0lEQVRLi5wNQV62d8OQ7XN/v7OhyE6H4/vBU/rD5aOa\nVjxS6HY59L3+zNaBMSEi7EI/r6iUKbNSiawnvHLLIBpGWcdtBaqQs+1EU83ORXBkl/Ncg2bQaSj0\nH+/sybfvD5FRNVdLZANoEe/cTsbjgYKcKjYOPkcTmcth/Uew9Uu44hk7CjBhK6xCX1V58P3VbD2Q\nx+w7hxAfax23eDywPx12LT4R9HnZznONWkHCcBj6E+dn295QLwg3kvXqOU0+jVtB215Vz+Mpg2/+\nBAunQlYa3DAT2vSo3TqNCQJhFfp/XbiVT9bt49HLezDinDDtuC0rgb1rTgT8rsVQeMR5rllHSLrQ\nCfiEEdCqa+iMqKkXASMfdo5UPpwMMy6CK56FATe5XZkxtSpsQn/hpv088/kmRvfrwKTzk9wup/aU\nFDp7tuVNNbuXQclx57mW50DP0dBpuBP0LTqFTshXp/NIuPtb+GgS/OMnsONba+6pKR4P5B+E3N1O\nv9D3t/LHWc4OR8uu0K63c5TWtpdzRNmkbeh/Fl0SFl+isvPQca568Vs6xjTio3uGh2Y7vioc3QMH\nM5zRLwczYN9aJ/DLigFx/qESvAHfabgz+iVc+Tb3tDrXmnvORPFxJ7irCvWjWc5zZUUVl6nfCJrH\nQ/M4aN7R6Sc6mAHZ651lyjVqeWID0Na7QWjdHepH1+7vWIf4+yUqIR/6x4tKufav35F9rJB/3nde\n3W/HLyl0OloPZsDBzd6f3qAvzjsxX1RTaNPdGVWTMAI6DYGGMe7VHay2LXSae4qOWXOPL0+Z07dT\nYc88s2LIF+RUXEbqQdP23kCPc5oLvw94761hTPV78Pk5Tv9S9npnGPC+dbB/A5QWeF8/wjk69T0i\naNfbeR87KrDQB6fj9r45K/lk7V5m3pHC+V1bB+R1a5wq5B86Eei+4X54J+DzN2veCVqd4+ytturq\n/XmuHR6fjmPZTnPP9m+g34TwaO4pynNGZVXYM/fdU9/zw2GwDZpXDPDmlUK9aXuIqB/YOj1lkLPd\n2Qhkrz+xQTiy88Q80c1PHA2Ubwza9Aj9v2ElFvrA9K+3MvWTjTxyWXfuurBLQF4zoMpKnBCvKtzL\nO1fBGQ/f8hyfUPf+bNkl7D7YNSZcmnuO7oVvnoYVsyqGer1IaNbhRIg36+gNc5+mmOjm7tVdWWGu\ncxRQYWOw3udoVyC2s08TkXeD0CLBGe0VgsI+9L/JOMDEN5ZxWZ/2vDR+gLsnYBUcOdHO7hvuOdu9\nlyvwatK20h6792ezuJD9oAadCs09z8CAm92uKDDyc2DR87B0hvOZG3grJJ53ItSbtA3O4binw+OB\n3F1Os1D5EUH2eqc5tPzoOKoptO1Z6aigJ0Q3c7X0QAjr0N91KJ+rXvqW9s2j+egnw2kUVYuDlPIO\nOCcB7d9wIujLx70D1Kvv7IH4NsW0OtdpogmmPalwFkrNPUV5sORl+O4vzoas743O0NXYMBrBVnwc\n9m/0bgR8NgiFuSfmaZEA7ftVvDVp417NZyBsQz+/2Om43ZtbyLz7RpDQspb+WQ9thcXTYNXbzgXF\noltA624Vw71lV4hJCHy7pwm8ut7cU1oEqW/Af55xLnbX7Qr40WPOXq7xjnbLOrEB2LsG9q3xHhV4\nNW1fcSPQrq9zVBSkfWX+hn5IjdNXVX794Vo2ZR/jzdtTaifws9Jg0V9gwzynXbTfOBh2nxMUQfrh\nMH6o8mSuOtDcU1YKa+Y6G6vc3ZB4Poyb41wjyZwgcqID+txLT0wvzHWah/auPnHb/LlzQUGAhrE/\nPCKISapTza8hFfqv/Wc7/1y9h4dGdePCc2twpI4qbPnCaSPd8R9nVMOI+2HI3c7FvUzo6DzS52Su\ne70ncz0bfM09qs6Ox5d/cJoUOwyA0S869dvOh/+im0PiCOdWrjjfOSLY57MhWDztRH9cVFNo37fi\nhqBl16C9BLhfzTsiMgp4AYgAXlPVqZWenwj8CSg/u+IlVX3N+9xtwGPe6X9Q1Zkne68zbd7Zsj+P\nS577mlG92zFtwsCa6bgtK4F1H8GiF2D/emjaAYb9BAbeFhIdQeYkKjf3XP9mcDSVqDoXkfvicdi7\nClp1c5pxelxlYV+TSovhwAafI4I1zsmQ5ecUREY7ncS+G4I2PZyLCNaQgLXpi0gEkAFcDGQCy4Hx\nqpruM89EIFlV76u0bCyQCiTjdJ+nAYNUtaoLpQNn16b/rzV7uKhbGxo3CPAWtijPGeK2eBoczYTW\nPWDEz6D3dTV7lUkTfLZ9DR9OCo7RPbuXwxf/4xxtNu8EFz3idNTW9VE4dZWnzBmZ59s0tG+N8/0R\n4AziaNPduxHo7/xs2ytgR42BbNNPAbao6jbvC88FxgDpJ13KcSkwX1VzvMvOB0YBc/xY9rRd2bdD\nYF8wbz8sfQWWv+aMm08YAVf+Gc65uE614ZkA6nyh+8092eudZpxN/3a+Seyyp2HQxBrdizR+qBfh\nhHqb7tDvRmeaxwNHdlTcEGz6BFa+5Twv9Zwjx3be5qGOgyBhWI2W6U/odwR2+zzOBIZUMd9YEbkA\n56jgF6q6u5plO55hrbXn0Fb47kVY9Y5z3ZoeV8Lw+60zzDiatoVbPva5VPOK2mnuydkOXz0Ja993\nrlnzo8dgyD3QoEnNvq85c/XqOUO0YztDr2ucaeXXyfLdEOz4Fta+Bx2TYfIXNVpSoNpB/gnMUdUi\nEbkLmAn8yN+FRWQKMAWgU6dOASrpDGSmOZ2zG/4JEVHOl4UM+6kzht4YX9+P7hnmNPe8+qOaa+45\ntg++fhpWzHSaCEbc79waxQb+vUzNE/FewqIjdL/8xPS8A87lV2qYP6GfBfh+dVEcJzpsAVBV30pf\nA572WXZkpWUXVn4DVZ0BzACnTd+PmgJHFTbPdzpnd37r9N6f/wCk3BXeV6E0/qnJ5p78HOdzufQV\n71m0t8EFD0Kz9mf/2ib4NGnt3GqYP6G/HOgqIkk4IT4OmOA7g4i0V9W93oejgQ3e+58BT4pI+eUd\nLwEeOeuqA6G0GNZ96JypuD/dudbIpU86p6c3aOp2daYuCXRzT1EeLH0ZFr3odAL2vcF7Fm3ngJZt\nwtMpQ19VS0XkPpwAjwBeV9X1IvI4kKqq84CfichooBTIASZ6l80RkSdwNhwAj5d36rqm6BikzYQl\nf3XOyGvTE655BXqPtTNlzZmrrrmn/03+D50sLYK0N52Nx/EDzhe5/+ix6r8C0pgzEHKXYajWsWxY\nOh2W/w2Kcp0zFUfcD+f8l41nNoFV4do940/d3OMpg9XlZ9Hucj6bP/4txKfUXs2mzgvLyzBU6eAW\npwln9Rzn5Kqeo52ROHGD3K7MhCp/m3tUnUEDX/4BDm5yxm6PfgE6X2Q7IqbGhG7oZ6bCt8/Bxv9z\nRuIMuNm5Jk7LILyuvgk9p2ru2fqVcxbtnhXeC7rNgh6jLexNjQut0Pd4nIsjffcX50vAo1vABb+C\nlCl17jKpJkRUHt2z9Ss4vt9p+mkeD2OmQd9xQXudFhN6QueTdngHvDPOuR5G83gYNRUG3GInrhj3\nVW7uadQSRj0FybfbWbSm1oVO6DeLc65Vf/4DzplvNhLHBJPy5p4+1zvfUmU7I8YloRP6EZEw4V23\nqzDm5KxPybjMrhpmjDFhxELfGGPCiIW+McaEEQt9Y4wJIxb6xhgTRiz0jTEmjFjoG2NMGLHQN8aY\nMBJ0l1YWkQPAzhp8i1bAwRp8/UCpK3VC3anV6gysulIn1J1az6bOBFU95VdvBV3o1zQRSfXnmtNu\nqyt1Qt2p1eoMrLpSJ9SdWmujTmveMcaYMGKhb4wxYSQcQ3+G2wX4qa7UCXWnVqszsOpKnVB3aq3x\nOsOuTd8YY8JZOO7pG2NM2ArZ0BeReBH5SkTSRWS9iNzvnf57EckSkVXe2+Vu1wogIjtEZK23plTv\ntFgRmS8im70/Y1yusZvPelslIkdF5OfBsk5F5HUR2S8i63ymVbkOxfEXEdkiImtEZKDLdf5JRDZ6\na/m7iLTwTk8UkQKfdTvd5Tqr/VuLyCPe9blJRC51uc53fWrcISKrvNPdXJ/VZVLtfkZVNSRvQHtg\noPd+UyAD6An8HviV2/VVUe8OoFWlaU8DD3vvPww85XadPrVFAPuAhGBZp8AFwEBg3anWIXA58Akg\nwFBgqct1XgJEeu8/5VNnou98QbA+q/xbe/+3VgMNgCRgKxDhVp2Vnn8W+G0QrM/qMqlWP6Mhu6ev\nqntVdYX3/jFgA9DR3apO2xhgpvf+TOBqF2up7MfAVlWtyRPpTouqfgPkVJpc3TocA8xSxxKghYi0\nd6tOVf1cVUu9D5cAcbVRy8lUsz6rMwaYq6pFqrod2AKk1FhxPk5Wp4gIcAMwpzZqOZmTZFKtfkZD\nNvR9iUgiMABY6p10n/dw6XW3m0x8KPC5iKSJyBTvtLaqutd7fx/Q1p3SqjSOiv9IwbhOofp12BHY\n7TNfJsGzU3AHzh5euSQRWSkiX4vI+W4V5aOqv3Wwrs/zgWxV3ewzzfX1WSmTavUzGvKhLyJNgA+B\nn6vqUeBloAvQH9iLc+gXDM5T1YHAZcC9InKB75PqHO8FxVArEYkCRgPveycF6zqtIJjWYXVE5FGg\nFHjbO2kv0ElVBwAPAO+ISDO36qOO/K19jKfizonr67OKTPpebXxGQzr0RaQ+zsp9W1U/AlDVbFUt\nU1UP8Cq1dAh6Kqqa5f25H/g7Tl3Z5Ydz3p/73auwgsuAFaqaDcG7Tr2qW4dZQLzPfHHeaa4RkYnA\nlcBN3n9+vM0lh7z303Days91q8aT/K2DcX1GAtcC75ZPc3t9VpVJ1PJnNGRD39uW9zdgg6r+2We6\nb5vYNcC6ysvWNhFpLCJNy+/jdOqtA+YBt3lnuw34hzsV/kCFvadgXKc+qluH84BbvSMkhgK5PofY\ntU5ERgEPAaNVNd9nemsRifDe7wx0Bba5U+VJ/9bzgHEi0kBEknDqXFbb9VXyX8BGVc0sn+Dm+qwu\nk6jtz6gbvdi1cQPOwzlMWgOs8t4uB2YDa73T5wHtg6DWzjgjH1YD64FHvdNbAl8Am4EFQGwQ1NoY\nOAQ095kWFOsUZ0O0FyjBaf+8s7p1iDMiYhrOnt5aINnlOrfgtN+Wf1ane+cd6/1MrAJWAFe5XGe1\nf2vgUe/63ARc5mad3ulvAndXmtfN9VldJtXqZ9TOyDXGmDASss07xhhjfshC3xhjwoiFvjHGhBEL\nfWOMCSMW+sYYE0Ys9I0xJoxY6BtjTBix0DfGmDDy/+FCISwL4ZJ2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGPA9pMaIp6d",
        "colab_type": "code",
        "outputId": "ffedec00-5a4e-4901-dab0-b93d923190c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Generate table showing Training & Test Data Accuracy\n",
        "df = pd.DataFrame(pd.concat([pd.Series(epoch_num),pd.Series(train_acc),pd.Series(test_acc)], axis=1))\n",
        "df.columns = ['Epoch','Training Data Accuracy','Test Data Acccuracy']\n",
        "df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Data Accuracy</th>\n",
              "      <th>Test Data Acccuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20</td>\n",
              "      <td>0.50484</td>\n",
              "      <td>0.4870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>0.56600</td>\n",
              "      <td>0.5095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>0.60400</td>\n",
              "      <td>0.5171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>0.64308</td>\n",
              "      <td>0.5289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>0.66758</td>\n",
              "      <td>0.5242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>120</td>\n",
              "      <td>0.62240</td>\n",
              "      <td>0.4876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>140</td>\n",
              "      <td>0.70384</td>\n",
              "      <td>0.5133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>160</td>\n",
              "      <td>0.76052</td>\n",
              "      <td>0.5190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>180</td>\n",
              "      <td>0.77230</td>\n",
              "      <td>0.5102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>200</td>\n",
              "      <td>0.74940</td>\n",
              "      <td>0.5007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Epoch  Training Data Accuracy  Test Data Acccuracy\n",
              "0     20                 0.50484               0.4870\n",
              "1     40                 0.56600               0.5095\n",
              "2     60                 0.60400               0.5171\n",
              "3     80                 0.64308               0.5289\n",
              "4    100                 0.66758               0.5242\n",
              "5    120                 0.62240               0.4876\n",
              "6    140                 0.70384               0.5133\n",
              "7    160                 0.76052               0.5190\n",
              "8    180                 0.77230               0.5102\n",
              "9    200                 0.74940               0.5007"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTCRPWTrYgF",
        "colab_type": "text"
      },
      "source": [
        "**MLP generated lower Test Data Accuracy than CNN (MLP @ 52.89%, CNN @ 78.57%)**. Since MLP's input is a one-dimensional vector (i.e. each pixel of the image is a feature), **model training is inefficient**. Multiple layers are needed before highly distinctive areas of the image can stand out. With only three layers, MLP pales in comparison to CNN when it comes to image data."
      ]
    }
  ]
}