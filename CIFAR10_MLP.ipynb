{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 - MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpagxIzidkaz",
        "colab_type": "text"
      },
      "source": [
        "# **Multi Layer Perceptron for CIFAR10**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0NwreZX97F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxc6VEqZ-F5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-4Dc_bhsbv",
        "colab_type": "text"
      },
      "source": [
        "Load CIFAR10 dataset and prepare training/test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66_HM-iC-kHu",
        "colab_type": "code",
        "outputId": "91b65362-78e2-4ce5-cdc5-ec95ff47db76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Get number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# Convert labels to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Get image dimension (image is assumed to be square)\n",
        "image_size = x_train.shape[1]\n",
        "\n",
        "# For MLP, the input dim is a vector, so we reshape\n",
        "input_size = image_size * image_size * 3\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "\n",
        "# Resize image and normalize\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYh89Z_5hv98",
        "colab_type": "text"
      },
      "source": [
        "Set network parameters that will be used by MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz27RNh-iDD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "hidden_units = [256,128,64]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpadK-yHiGNY",
        "colab_type": "text"
      },
      "source": [
        "The MLP Architecture has **three Dense layers** with decreasing number of nodes (256-128-64). The **ReLU** activation function is added after each Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpzNeAx4qmk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MLP Architecture\n",
        "# Dense-Dense-Dense-Dense-Activation\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units[0], input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units[1]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units[2]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcjBxOSUq35W",
        "colab_type": "text"
      },
      "source": [
        "The MLP is trained over **200 epochs**. For every 20th epoch, Training & Test Data Accuracies are generated for evaluation. **Categorical Crossentropy** is chosen as the loss function and **Accuracy** is chosen as the metric since the MLP will perform **single label classification** (i.e. only one label can be correct)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja4lFWNKB1jC",
        "colab_type": "code",
        "outputId": "79d72ce2-ae02-4510-fc41-265122dce7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the MLP\n",
        "i = 1\n",
        "epoch = 20\n",
        "epoch_num = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "while i <= 10:\n",
        "  epoch_num.append(epoch * i)\n",
        "  model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
        "  train_score = model.evaluate(x_train, y_train, batch_size=batch_size)\n",
        "  print(\"\\nTrain accuracy: %.1f%%\" % (100.0 * train_score[1]))\n",
        "  train_acc.append(train_score[1])\n",
        "  test_score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "  print(\"\\nTest accuracy: %.1f%%\" % (100.0 * test_score[1]))\n",
        "  test_acc.append(test_score[1])\n",
        "  i = i + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 828,490\n",
            "Trainable params: 828,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 56us/sample - loss: 2.0362 - acc: 0.2705\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 40us/sample - loss: 1.8350 - acc: 0.3492\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 40us/sample - loss: 1.7548 - acc: 0.3799\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.7029 - acc: 0.3974\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6590 - acc: 0.4136\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6268 - acc: 0.4268\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.6034 - acc: 0.4337\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5729 - acc: 0.4435\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5509 - acc: 0.4513\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.5279 - acc: 0.4604\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.5103 - acc: 0.4657\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4885 - acc: 0.4746\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4755 - acc: 0.4792\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4549 - acc: 0.4867\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4443 - acc: 0.4890\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4236 - acc: 0.4951\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.4134 - acc: 0.4997\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.4006 - acc: 0.5035\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3876 - acc: 0.5102\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3727 - acc: 0.5150\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.3474 - acc: 0.5255\n",
            "\n",
            "Train accuracy: 52.6%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.4241 - acc: 0.4899\n",
            "\n",
            "Test accuracy: 49.0%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3593 - acc: 0.5185\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3504 - acc: 0.5224\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.3371 - acc: 0.5267\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3267 - acc: 0.5314\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3164 - acc: 0.5358\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.3044 - acc: 0.5412\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.3026 - acc: 0.5410\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2825 - acc: 0.5476\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.2780 - acc: 0.5501\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2652 - acc: 0.5547\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.2592 - acc: 0.5556\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.2517 - acc: 0.5577\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2383 - acc: 0.5615\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2322 - acc: 0.5655\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2243 - acc: 0.5688\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.2157 - acc: 0.5716\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.2049 - acc: 0.5750\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.2002 - acc: 0.5754\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1963 - acc: 0.5787\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1802 - acc: 0.5847\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.1464 - acc: 0.5966\n",
            "\n",
            "Train accuracy: 59.7%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.3509 - acc: 0.5205\n",
            "\n",
            "Test accuracy: 52.1%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1707 - acc: 0.5904\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1680 - acc: 0.5892\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1582 - acc: 0.5913\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1467 - acc: 0.5962\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1451 - acc: 0.5970\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1406 - acc: 0.5974\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1267 - acc: 0.6025\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.1217 - acc: 0.6041\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1121 - acc: 0.6083\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.1097 - acc: 0.6080\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0955 - acc: 0.6144\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0927 - acc: 0.6150\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0815 - acc: 0.6193\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0811 - acc: 0.6181\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0781 - acc: 0.6216\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0634 - acc: 0.6272\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0602 - acc: 0.6278\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0502 - acc: 0.6328\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0435 - acc: 0.6325\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0370 - acc: 0.6357\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 1.0417 - acc: 0.6343\n",
            "\n",
            "Train accuracy: 63.4%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.3928 - acc: 0.5214\n",
            "\n",
            "Test accuracy: 52.1%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0344 - acc: 0.6351\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0250 - acc: 0.6393\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0157 - acc: 0.6425\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 1.0100 - acc: 0.6461\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 1.0096 - acc: 0.6459\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9993 - acc: 0.6477\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9877 - acc: 0.6532\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9817 - acc: 0.6548\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9766 - acc: 0.6560\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9728 - acc: 0.6566\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9666 - acc: 0.6581\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9568 - acc: 0.6648\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9492 - acc: 0.6639\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9469 - acc: 0.6686\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9386 - acc: 0.6691\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9350 - acc: 0.6718\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9307 - acc: 0.6721\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.9177 - acc: 0.6785\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9133 - acc: 0.6797\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9107 - acc: 0.6805\n",
            "50000/50000 [==============================] - 1s 29us/sample - loss: 0.8689 - acc: 0.6963\n",
            "\n",
            "Train accuracy: 69.6%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.3837 - acc: 0.5340\n",
            "\n",
            "Test accuracy: 53.4%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.9014 - acc: 0.6843\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8966 - acc: 0.6840\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8867 - acc: 0.6890\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8863 - acc: 0.6901\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8723 - acc: 0.6918\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8703 - acc: 0.6942\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8707 - acc: 0.6937\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8603 - acc: 0.7002\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8526 - acc: 0.6997\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8486 - acc: 0.7029\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8387 - acc: 0.7056\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8347 - acc: 0.7074\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.8276 - acc: 0.7095\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8250 - acc: 0.7080\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8158 - acc: 0.7144\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8083 - acc: 0.7161\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8009 - acc: 0.7195\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.8103 - acc: 0.7147\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7998 - acc: 0.7205\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.7823 - acc: 0.7262\n",
            "50000/50000 [==============================] - 1s 29us/sample - loss: 0.7628 - acc: 0.7334\n",
            "\n",
            "Train accuracy: 73.3%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.4609 - acc: 0.5258\n",
            "\n",
            "Test accuracy: 52.6%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7856 - acc: 0.7245\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7764 - acc: 0.7292\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7709 - acc: 0.7282\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7666 - acc: 0.7309\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.7625 - acc: 0.7325\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.7537 - acc: 0.7349\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.7494 - acc: 0.7389\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7479 - acc: 0.7390\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7443 - acc: 0.7391\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.7341 - acc: 0.7434\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.7208 - acc: 0.7469\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7210 - acc: 0.7480\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7147 - acc: 0.7504\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.7074 - acc: 0.7524\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.7000 - acc: 0.7545\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6975 - acc: 0.7580\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6960 - acc: 0.7559\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6892 - acc: 0.7595\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6752 - acc: 0.7639\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6713 - acc: 0.7640\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 0.9152 - acc: 0.6867\n",
            "\n",
            "Train accuracy: 68.7%\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 1.8597 - acc: 0.4775\n",
            "\n",
            "Test accuracy: 47.7%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6694 - acc: 0.7655\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6694 - acc: 0.7669\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6623 - acc: 0.7671\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6557 - acc: 0.7718\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6570 - acc: 0.7674\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6435 - acc: 0.7761\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6432 - acc: 0.7741\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6345 - acc: 0.7774\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6283 - acc: 0.7810\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6221 - acc: 0.7827\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6167 - acc: 0.7853\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6123 - acc: 0.7858\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6174 - acc: 0.7844\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.6019 - acc: 0.7892\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.6039 - acc: 0.7900\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5937 - acc: 0.7943\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5943 - acc: 0.7925\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5783 - acc: 0.7987\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5822 - acc: 0.7968\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5701 - acc: 0.8026\n",
            "50000/50000 [==============================] - 1s 29us/sample - loss: 0.5360 - acc: 0.8123\n",
            "\n",
            "Train accuracy: 81.2%\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.6908 - acc: 0.5215\n",
            "\n",
            "Test accuracy: 52.1%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5735 - acc: 0.8002\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5633 - acc: 0.8022\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5688 - acc: 0.8022\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5539 - acc: 0.8063\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5643 - acc: 0.8026\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5546 - acc: 0.8055\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5378 - acc: 0.8122\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 0.5394 - acc: 0.8120\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 0.5369 - acc: 0.8140\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5182 - acc: 0.8206\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5317 - acc: 0.8120\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5175 - acc: 0.8200\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.5166 - acc: 0.8198\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 36us/sample - loss: 0.5085 - acc: 0.8231\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.5225 - acc: 0.8170\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4967 - acc: 0.8274\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4873 - acc: 0.8310\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 0.4973 - acc: 0.8271\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 39us/sample - loss: 0.4873 - acc: 0.8311\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 40us/sample - loss: 0.4722 - acc: 0.8360\n",
            "50000/50000 [==============================] - 1s 29us/sample - loss: 0.5171 - acc: 0.8151\n",
            "\n",
            "Train accuracy: 81.5%\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 1.9477 - acc: 0.5166\n",
            "\n",
            "Test accuracy: 51.7%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 41us/sample - loss: 0.4725 - acc: 0.8367\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.4661 - acc: 0.8397\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4618 - acc: 0.8410\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.4652 - acc: 0.8395\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4717 - acc: 0.8366\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.4500 - acc: 0.8443\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4470 - acc: 0.8464\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4492 - acc: 0.8445\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4647 - acc: 0.8405\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4280 - acc: 0.8531\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4264 - acc: 0.8544\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4472 - acc: 0.8462\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.4272 - acc: 0.8536\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4274 - acc: 0.8521\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4136 - acc: 0.8595\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4148 - acc: 0.8560\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4166 - acc: 0.8546\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3989 - acc: 0.8632\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4040 - acc: 0.8603\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.4054 - acc: 0.8600\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 0.3510 - acc: 0.8816\n",
            "\n",
            "Train accuracy: 88.2%\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 2.0471 - acc: 0.5157\n",
            "\n",
            "Test accuracy: 51.6%\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.4017 - acc: 0.8629\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3919 - acc: 0.8657\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3843 - acc: 0.8699\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3705 - acc: 0.8728\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3931 - acc: 0.8640\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3794 - acc: 0.8714\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3539 - acc: 0.8806\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3701 - acc: 0.8725\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3507 - acc: 0.8800\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3732 - acc: 0.8719\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3778 - acc: 0.8725\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3577 - acc: 0.8773\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3383 - acc: 0.8853\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3553 - acc: 0.8810\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3438 - acc: 0.8832\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 2s 38us/sample - loss: 0.3428 - acc: 0.8853\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3333 - acc: 0.8868\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3427 - acc: 0.8849\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3262 - acc: 0.8896\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3495 - acc: 0.8833\n",
            "50000/50000 [==============================] - 1s 28us/sample - loss: 0.2841 - acc: 0.9050\n",
            "\n",
            "Train accuracy: 90.5%\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 2.2520 - acc: 0.5176\n",
            "\n",
            "Test accuracy: 51.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyhu42syrO3U",
        "colab_type": "text"
      },
      "source": [
        "The model's Training and Test Data Accuracy is plotted against the number of epochs performed during training. Looking at the plot, it can be observed that **as the number of epochs increase, Training Data Accuracy increases**. On the other hand, **Test Data Accuracy gradually decreases after 80 epochs**, indicating **model overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rtt3DxnsPms",
        "colab_type": "code",
        "outputId": "27c8752c-b505-4d9e-be21-adebe9387006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Generate plot showing Training & Test Data Accuracy\n",
        "plt.plot(epoch_num, train_acc)\n",
        "plt.plot(epoch_num, test_acc)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGPA9pMaIp6d",
        "colab_type": "code",
        "outputId": "8ffb2bbf-7aca-4525-eded-f59e4dd78c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Generate table showing Training & Test Data Accuracy\n",
        "df = pd.DataFrame(pd.concat([pd.Series(epoch_num),pd.Series(train_acc),pd.Series(test_acc)], axis=1))\n",
        "df.columns = ['Epoch','Training Data Accuracy','Test Data Acccuracy']\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20</td>\n",
              "      <td>0.52552</td>\n",
              "      <td>0.4899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>0.59664</td>\n",
              "      <td>0.5205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>0.63434</td>\n",
              "      <td>0.5214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>0.69628</td>\n",
              "      <td>0.5340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>0.73336</td>\n",
              "      <td>0.5258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>120</td>\n",
              "      <td>0.68666</td>\n",
              "      <td>0.4775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>140</td>\n",
              "      <td>0.81230</td>\n",
              "      <td>0.5215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>160</td>\n",
              "      <td>0.81510</td>\n",
              "      <td>0.5166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>180</td>\n",
              "      <td>0.88160</td>\n",
              "      <td>0.5157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>200</td>\n",
              "      <td>0.90502</td>\n",
              "      <td>0.5176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0        1       2\n",
              "0   20  0.52552  0.4899\n",
              "1   40  0.59664  0.5205\n",
              "2   60  0.63434  0.5214\n",
              "3   80  0.69628  0.5340\n",
              "4  100  0.73336  0.5258\n",
              "5  120  0.68666  0.4775\n",
              "6  140  0.81230  0.5215\n",
              "7  160  0.81510  0.5166\n",
              "8  180  0.88160  0.5157\n",
              "9  200  0.90502  0.5176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTCRPWTrYgF",
        "colab_type": "text"
      },
      "source": [
        "**MLP generated lower Test Data Accuracy than CNN (MLP @ 53.40%, CNN @ 75.04%)**. Since MLP's input is a one-dimensional vector (i.e. each pixel of the image is a feature), **model training is inefficient**. Multiple layers are needed before highly distinctive areas of the image can stand out. With only three layers, MLP pales in comparison to CNN when it comes to image data."
      ]
    }
  ]
}